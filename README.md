# ğŸš¦ AnÃ¡lisis de Movilidad BogotÃ¡ con Spark y Kafka

## ğŸ“– IntroducciÃ³n
Este proyecto analiza datos de movilidad en BogotÃ¡ utilizando **Apache Spark** y **Apache Kafka** para realizar procesamiento **batch** y **en tiempo real (streaming)**.  

El conjunto de datos contiene informaciÃ³n sobre los medios de transporte utilizados por los ciudadanos en sus viajes diarios, permitiendo identificar patrones de desplazamiento, tiempos promedio y comportamientos de movilidad urbana.

---

## ğŸ—ºï¸ DefiniciÃ³n del problema y conjunto de datos

El propÃ³sito del proyecto es **analizar los patrones de movilidad de los ciudadanos de BogotÃ¡** a partir de encuestas sobre viajes, con el fin de identificar:

- Los **medios de transporte mÃ¡s utilizados**.  
- Los **horarios de mayor flujo de viajes**.  
- Los **tiempos promedio de desplazamiento** segÃºn el medio de transporte.  

El conjunto de datos utilizado proviene de la **Encuesta de Movilidad de BogotÃ¡ (Encuesta_movilidad.csv)**, disponible pÃºblicamente en **Kaggle** y en el portal de **Datos Abiertos de Colombia**.  

Columnas principales:
- `ID_ENCUESTA`: identificador del participante  
- `NUMERO_PERSONA`: persona dentro de la encuesta  
- `NUMERO_VIAJE`: nÃºmero de viaje realizado  
- `MOTIVOVIAJE`: motivo del viaje (trabajo, estudio, ocio, etc.)  
- `MUNICIPIO_DESTINO`, `DEPARTAMENTO_DESTINO`: destino del viaje  
- `TIEMPO_CAMINO`: duraciÃ³n del viaje  
- `HORA_INICIO`, `HORA_FIN`: hora de inicio y fin del viaje  
- `MEDIO_PREDOMINANTE`: medio de transporte principal  

---

## âš™ï¸ Arquitectura de la soluciÃ³n

La arquitectura incluye los siguientes componentes principales:

- **Kafka ğŸ“¨:** para la transmisiÃ³n de mensajes en tiempo real.  
- **Spark Structured Streaming âš™ï¸:** para el procesamiento continuo de los datos recibidos desde Kafka.  
- **Python ğŸ:** para generar los mensajes (productor) y procesarlos (consumidor).  
- **Hadoop y VirtualBox ğŸ–¥ï¸:** entorno distribuido configurado dentro de una mÃ¡quina virtual Ubuntu Server.  

---

## ğŸ“ Archivos principales

| Archivo | DescripciÃ³n |
|----------|-------------|
| `batch_process.py` | Realiza el anÃ¡lisis inicial del dataset en modo batch (limpieza, transformaciÃ³n y cÃ¡lculo de promedios). |
| `kafka_producer.py` | EnvÃ­a los registros del dataset al tÃ³pico de Kafka (`movilidad`). |
| `spark_streaming_consumer.py` | Consume los mensajes desde Kafka y los procesa en tiempo real con Spark Streaming. |
| `Encuesta_movilidad.csv` | Dataset base con la informaciÃ³n de los registros. |

---

## ğŸš€ Procesamiento Batch (Apache Spark)

### ğŸ“Œ DescripciÃ³n
El procesamiento batch se encarga de analizar los datos histÃ³ricos de movilidad en BogotÃ¡.  
Permite identificar los medios de transporte mÃ¡s utilizados y los tiempos promedio de desplazamiento.

### âš™ï¸ EjecuciÃ³n del script
cd ~/datasets/encuesta_movilidad
spark-submit batch_process.py

ğŸ’¡ AsegÃºrate de haber iniciado los servicios de Hadoop y configurado correctamente las variables de entorno de Spark antes de ejecutar el comando.

ğŸ§® Proceso realizado
Carga del dataset desde un archivo CSV.

Limpieza y transformaciÃ³n de datos.

AgrupaciÃ³n y anÃ¡lisis de los viajes por medio de transporte.

GeneraciÃ³n de resultados en consola y en archivos CSV.

ğŸ“ Resultados
Los resultados se almacenan en:
~/datasets/encuesta_movilidad/resultados/

ğŸ” Procesamiento en Tiempo Real (Streaming)
El procesamiento en tiempo real se realiza con Kafka + Spark Streaming, simulando la llegada de datos dinÃ¡micos.

Kafka Producer: (kafka_producer.py) envÃ­a los registros del dataset al tÃ³pico movilidad.

Spark Streaming Consumer: (spark_streaming_consumer.py) recibe los datos, calcula promedios y totales por tipo de transporte, y muestra resultados en consola.

â–¶ï¸ EjecuciÃ³n del proyecto

1ï¸âƒ£ Iniciar Zookeeper y Kafka:
$KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
$KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties

2ï¸âƒ£ Ejecutar el productor:
python3 kafka_producer.py

3ï¸âƒ£ Ejecutar el consumidor (Spark Streaming):
spark-submit spark_streaming_consumer.py

4ï¸âƒ£ Detener los procesos:
Ctrl + C para detener el producer y consumer.
Cerrar Zookeeper y Kafka si es necesario.

ğŸ“Š VisualizaciÃ³n de resultados
Ejemplo de salida en tiempo real (Spark Streaming):

+-------------+-----------------+----------------+
| MEDIO_PRE   | PROMEDIO_TIEMPO | TOTAL_VIAJES  |
+-------------+-----------------+----------------+
| Transmilenio| 43.2            | 235            |
| Bus Urbano  | 39.7            | 204            |
| Bicicleta   | 27.1            | 128            |
| PeatÃ³n      | 25.3            | 226            |
| AutomÃ³vil   | 35.4            | 142            |
+-------------+-----------------+----------------+
ğŸ“¸ Evidencias del Proyecto
### ğŸ§¾ 1. Procesamiento Batch 
EjecuciÃ³n del script batch_process.py para procesar el conjunto de datos y generar los resultados agregados. 
![Procesamiento Batch](evidencias/01_datos_batch_process.png) 
--- 
### ğŸ“Š 2. Resultados del Procesamiento Batch 
VisualizaciÃ³n del archivo generado con los conteos por tipo de medio de transporte. 
![Resultados CSV](evidencias/02_resultados_batch_csv.png) 
--- 
### ğŸš€ 3. Productor Kafka 
Enviando Datos SimulaciÃ³n del flujo de datos en tiempo real desde el productor hacia el tÃ³pico de Kafka. 
![Productor Kafka](evidencias/03_kafka_productor.png) 
--- 
### âš™ï¸ 4. Spark Streaming en EjecuciÃ³n 
EjecuciÃ³n del proceso spark_streaming_consumer.py mostrando los resultados del anÃ¡lisis en tiempo real. 
![Spark Streaming](evidencias/04_spark_streaming.png) 
--- 
### ğŸ’» 5. Repositorio en GitHub 
Estructura final del repositorio con el cÃ³digo fuente, scripts y documentaciÃ³n del proyecto. 
![Repositorio GitHub](evidencias/05_github_repo.png) 
---


ğŸ§  Conclusiones
El proyecto permitiÃ³ integrar procesamiento batch y en tiempo real utilizando herramientas del ecosistema Big Data.
Se lograron identificar patrones de movilidad, tiempos promedio por transporte y horas pico, mostrando el potencial del anÃ¡lisis de datos masivos en la planificaciÃ³n urbana.


ğŸ§‘â€ğŸ’» Autora
Proyecto desarrollado por:
Andrea Gordillo Rojas
Universidad Nacional Abierta y a Distancia (UNAD)
Curso: Big Data â€“ 2025

ğŸ”— Enlaces
ğŸ“‚ Repositorio del proyecto: GitHub - Proyecto Spark Movilidad
ğŸ¥ Video explicativo: Enlace pendiente
ğŸ“‘ PresentaciÃ³n del proyecto: Enlace pendiente
