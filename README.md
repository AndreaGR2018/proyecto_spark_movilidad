# ğŸš¦ Proyecto de AnÃ¡lisis de Movilidad con Spark y Kafka

## ğŸ“– IntroducciÃ³n
Este proyecto realiza el **anÃ¡lisis de datos de movilidad en BogotÃ¡** utilizando **Apache Spark** y **Apache Kafka**, aplicando tanto **procesamiento batch** como **procesamiento en tiempo real (streaming)**.  

El objetivo principal es demostrar cÃ³mo integrar ambas modalidades de anÃ¡lisis sobre un mismo dominio de datos, partiendo de un conjunto de informaciÃ³n pÃºblica y simulando el flujo continuo de datos en Kafka.

---

## ğŸ¯ DefiniciÃ³n del Problema

La movilidad en BogotÃ¡ involucra grandes volÃºmenes de datos derivados de los viajes diarios de los ciudadanos.  
El reto consiste en **analizar estos datos para obtener patrones de uso de medios de transporte y tiempos de desplazamiento promedio**, tanto de forma histÃ³rica (batch) como simulada en tiempo real (streaming).

---

## ğŸ§© Conjunto de Datos

- **Fuente:** [Kaggle â€“ Encuesta de Movilidad BogotÃ¡](https://www.kaggle.com/)  
- **Archivo:** `Encuesta_movilidad.csv`
- **UbicaciÃ³n en el entorno:**  
  `/home/vboxuser/datasets/Encuesta_movilidad.csv`
- **DescripciÃ³n de columnas relevantes:**
  - `MEDIO_PREDOMINANTE`: Medio de transporte principal usado.
  - `TIEMPO_CAMINO`: Tiempo total de desplazamiento.
  - Otras variables relacionadas con los viajes y caracterÃ­sticas de los encuestados.

---

## âš™ï¸ TecnologÃ­as Utilizadas

- **Apache Spark 3.5.3**
- **Apache Kafka 3.6.2**
- **Python 3.10**
- **Hadoop (modo local)**
- **Sistema Operativo:** Ubuntu (mÃ¡quina virtual `vboxuser`)

---

## ğŸ§® Procesamiento Batch (`batch_process.py`)

El procesamiento batch carga y limpia el dataset completo, calcula estadÃ­sticas descriptivas y almacena los resultados en formato CSV.

### ğŸ”¹ Pasos principales:
1. **Carga del dataset desde la ruta local.**
2. **Limpieza:**  
   - EliminaciÃ³n de registros nulos.  
   - Filtrado de tiempos de viaje invÃ¡lidos.
3. **CÃ¡lculos realizados:**
   - Conteo de viajes por medio de transporte.
   - Promedio de tiempo de viaje por medio de transporte.
4. **Salida de resultados:**  
   Los resultados se almacenan en:
/home/vboxuser/datasets/encuesta_movilidad/resultados/
â”œâ”€â”€ conteo_viajes/
â””â”€â”€ promedio_tiempo/


### ğŸ§  EjecuciÃ³n del procesamiento batch
```bash
spark-submit batch_process.py
âš¡ Procesamiento en Tiempo Real (Spark Streaming + Kafka)
El flujo en tiempo real simula la llegada continua de datos mediante Kafka y los procesa usando Spark Structured Streaming.

ğŸ§± Arquitectura
java
Copiar cÃ³digo
Kafka Producer  â†’  Kafka Topic (sensor_data)  â†’  Spark Streaming Consumer
ğŸš€ ConfiguraciÃ³n y EjecuciÃ³n Paso a Paso
ğŸ’¡ Importante: Todos los comandos se ejecutan como usuario vboxuser.

1ï¸âƒ£ Iniciar servicios

# Iniciar ZooKeeper
sudo /opt/Kafka/bin/zookeeper-server-start.sh /opt/Kafka/config/zookeeper.properties &

# Iniciar Kafka
sudo /opt/Kafka/bin/kafka-server-start.sh /opt/Kafka/config/server.properties &

2ï¸âƒ£ Crear el topic de Kafka

/opt/Kafka/bin/kafka-topics.sh --create \
  --bootstrap-server localhost:9092 \
  --replication-factor 1 \
  --partitions 1 \
  --topic sensor_data

3ï¸âƒ£ Ejecutar el productor (kafka_producer.py)
Este script genera datos simulados (por ejemplo, sensores o viajes) y los envÃ­a al topic sensor_data.

python3 kafka_producer.py
Ejemplo de salida:

yaml
Copiar cÃ³digo
Sent: {'sensor_id': 3, 'temperature': 27.1, 'humidity': 53.4, 'timestamp': 1729282738}

4ï¸âƒ£ Ejecutar el consumidor (spark_streaming_consumer.py)
En otra terminal:

spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 \
  spark_streaming_consumer.py
Ejemplo de salida en consola:

pgsql
Copiar cÃ³digo
+------------------------------------------+----------+------------------+------------------+
|window                                    |sensor_id |avg(temperature)  |avg(humidity)     |
+------------------------------------------+----------+------------------+------------------+
|{2025-10-26 23:35:00, 2025-10-26 23:36:00}|3         |27.4              |52.8              |
+------------------------------------------+----------+------------------+------------------+
ğŸ’» Puedes visualizar los Jobs y Stages activos en Spark en:
http://localhost:4040

ğŸ“Š Evidencias
En la carpeta /evidencias/ del repositorio se incluyen capturas del procesamiento en:

Modo batch (resultados CSV).

Modo streaming (Kafka + Spark Streaming funcionando en tiempo real).

ğŸ§° Estructura del Repositorio

proyecto_spark_movilidad/
â”œâ”€â”€ batch_process.py
â”œâ”€â”€ kafka_producer.py
â”œâ”€â”€ spark_streaming_consumer.py
â”œâ”€â”€ evidencias/
â”‚   â”œâ”€â”€ batch_resultados.png
â”‚   â”œâ”€â”€ streaming_kafka.png
â”‚   â””â”€â”€ spark_ui.png
â””â”€â”€ README.md
âœ… Resultados Obtenidos
Batch: identificaciÃ³n de los medios de transporte mÃ¡s usados y sus tiempos promedio.

Streaming: monitoreo continuo de eventos simulados en tiempo real, cÃ¡lculo de promedios por ventana temporal.

IntegraciÃ³n completa de Spark y Kafka, ejecutada en entorno local (sin Docker).

ğŸ§¾ Conclusiones
El proyecto demuestra la viabilidad de combinar procesamiento batch y en tiempo real en un entorno de Big Data usando Spark y Kafka.
AdemÃ¡s, la arquitectura propuesta puede adaptarse fÃ¡cilmente a escenarios reales de movilidad o IoT para analizar flujos masivos de informaciÃ³n.

ğŸ‘©â€ğŸ’» Autora
Andrea Gordillo
Proyecto desarrollado para la asignatura Big Data y AnÃ¡lisis en Tiempo Real.
Universidad Nacional Abierta y a Distancia UNAD â€” 2025.
