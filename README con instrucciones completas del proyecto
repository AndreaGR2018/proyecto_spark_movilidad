Proyecto de An谩lisis de Movilidad con Spark y Kafka

Introducci贸n
Este proyecto analiza datos de movilidad en Bogot谩 utilizando **Apache Spark** y **Apache Kafka** para el procesamiento en tiempo real.  
El conjunto de datos contiene informaci贸n sobre los medios de transporte utilizados por los ciudadanos en sus viajes diarios.

---

Arquitectura de la Soluci贸n

La arquitectura incluye los siguientes componentes:

- **Kafka** : para la transmisi贸n de mensajes en tiempo real.  
- **Spark Structured Streaming** 锔: para el procesamiento continuo de los datos recibidos desde Kafka.  
- **Docker Compose** : para levantar los servicios de Kafka y Zookeeper f谩cilmente.  
- **Python** : para generar los mensajes (productor) y procesarlos (consumidor).

---

Archivos Principales

| Archivo | Descripci贸n |
|----------|-------------|
| `producer_generator.py` | Env铆a los registros del dataset al t贸pico de Kafka (`movilidad_bogota`). |
| `streaming_kafka.py` | Consume los mensajes desde Kafka y los procesa en tiempo real con Spark. |
| `docker-compose.yml` | Define los contenedores de **Kafka** y **Zookeeper**. |
| `Encuesta_movilidad.csv` | Dataset base con la informaci贸n de los registros. |

---

Flujo del Sistema

1. **Kafka** recibe los mensajes generados por el productor (`producer_generator.py`).  
2. **Spark Streaming** (en `streaming_kafka.py`) consume esos mensajes desde el t贸pico.  
3. Los datos se agrupan por tipo de medio de transporte (`MEDIO_PRE`) y se muestran los resultados en tiempo real.  

---

Resultados Esperados

El sistema muestra en tiempo real la cantidad de viajes por tipo de medio de transporte (`MEDIO_PRE`) en batches de procesamiento continuo, por ejemplo:

+------------+-----+
| MEDIO_PRE |count|
+------------+-----+
| transmilenio |235 |
| bus |204 |
| bicicleta |128 |
| peaton |226 |
| carro |142 |
| taxi |80 |
| moto |135 |
+------------+-----+

---

Ejecuci贸n del Proyecto

1. Levantar los contenedores de Kafka y Zookeeper
Desde el directorio del proyecto:
```bash
sudo docker-compose up -d

2. Ejecutar el Productor (env铆a datos a Kafka)
python3 producer_generator.py

3. Ejecutar el Consumidor (procesa los datos en Spark)

En otra terminal:

spark-submit streaming_kafka.py

Requisitos Previos

Docker y Docker Compose instalados

Apache Spark configurado

Python 3.8 o superior

Librer铆as:

pip install kafka-python pyspark

Autores

Proyecto desarrollado por Andrea Gordillo Rojas
Universidad Nacional Abierta y a Distancia
Curso: Big Data
2025
